{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais 01\n",
    "\n",
    "Iremos abordar problemas antes não tão tangíveis sem redes neurais: NLP (Processamento de Linguagem Natural, ensinar computadores a entender e reproduzir linguagem humana) e Visão Computacional ( ensinar computadores a enxergar o mundo como energamos e tomar decisões com base em padrões visuais).\n",
    "\n",
    "## Alguns detalhes antes da aula\n",
    "\n",
    "Estudar Deep Learning tem alguns desdobramentos devido à natureza do assunto. Você pode pular essa seção se já vem estudando o assunto há um tempo, mas creio que isso poupará muita dor de cabeça para os iniciantes. <br>\n",
    "Primeiramente, vamos falar sobre ferramentas. Há diversas bibliotecas para desenvolvimento de redes neurais: TensorFlow, TensorFlow 2.0 (Google), Keras, Theano, Caffe, PyTorch, MXNet etc. Embora elas tenham diferenças entre si, desde sintaxe até performance, basicamente o importante é dominar a biblioteca que você escolher. Entendendo bem os princípios do funcionamento de redes neurais, a transição entre bibliotecas fica muito mais fluida. Alguns papers são esritos en PyTorch, outros em Keras, mas o importante é ter uma noção mínima para saber ler qualquer um. Recentemente, o Google disponibilizou a versão Alpha do TensorFlow 2.0, que é mais intuitiva e completa para se utilizar. Caso você queira rodar esse notebook na sua máquina, segue aqui o __[link para instalação geral do TensorFlow 2.0](https://www.tensorflow.org/install/pip)__ e o __[link para instalação do TensorFlow 2.0 Alpha com GPU](https://www.tensorflow.org/install/gpu)__. Falando em GPU, vamos para o próximo tópico: hardware. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que é uma Rede Neural?\n",
    "\n",
    "OK, vamos a uma pergunta um pouco mais profunda para entender de onde Redes Neurais, ou Artificial Neural Networks (ANNs), vieram. No campo de machine learning, nós queremos desenvolver algoritmos e estruturas que conseguem aprender com experiências e com isso obter uma performance desejada na execução de uma certa tarefa. O que já faz isso na natureza? Bem, parando pra pensar, nossos cérebros fazem justamente isso! Somos máquinas de aprendizado, então faz sentido se inspirar na nossa estrutura biológica para, então, construir uma nova tecnologia capaz de aprender. Então vamos tentar entender um neurônio biológico:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/neuron.png\" align=\"right\" width=\"50%\">\n",
    "Representação de um neurônio biológico. Num neurônio, temos basicamente canais de input (dendritos), um centro de processamento desse input (corpo celular), uma via de propagação de sinal elétrico (axônio) e canais de output (terminais do axônio). Isso parece muito com alguns pipelines que já vimos nesse curso! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, então faz sentido pensar que a estrutura fundamental do nosso sistema nervoso, o neurônio, seja capaz de realizar algum tipo de aprendizado. Pensando nisso, Frank Rosenblatt desenvolveu o modelo mais básico de rede neural, que consiste de um único neurônio, chamado **Perceptron**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/perceptron.jpg\" align=\"right\" width=\"50%\">\n",
    "\n",
    "\n",
    "Análogo ao neurônio, vemos canais de input xi, que são somados de acordo com um conjunto de pesos wi. Eles passam por uma **função de ativação** no corpo celular (no caso, uma step function) e então saem por um output. A esse movimento dos inputs, sendo multiplicados por seus pesos e passando pela função de ativação como uma combinação linear damos o nome de movimento **feedfoward**. O machine learning por trás de um perceptron torna-se, então, um problema de otimização dos pesos wi. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, então precisamos entender como escolher os pesos wi certos para que o output seja correto. Queremos entender como que a variação de cada peso wi afetará o output, além de termos que atualizar os pesos de acordo com as conclusões que chegarmos. Se temos uma função custo, uma **loss function** definida (RMSE, MAE etc), podemos, então, aplicar um algoritmo já bem conhecido por nós em machine learning: o gradiente descendente! Ao movimento de volta do sinal, comparando os outputs y com nossas predições yhat e atualizando os pesos de cada conexão input de neurônios, damos o nome de **backpropagation**. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/neural_net_inner_working.gif\" align=\"right\" width=\"50%\">\n",
    "\n",
    "Aqui podemos ver o funcionamento de uma rede neural genérica. Repare na ordem dos acontecimentos. Um input entra e realiza o seu movimento de feedfoward. Resultados são gerados e comparados com os valores reais. Então, o sinal faz o caminho contrário na rede, atualizando os valores dos pesos num movimento de backpropagation. A cada ciclo desses, damos o nome de **epoch**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Entendemos então como a rede neural mais básica funciona. Bem, ela não é beeeem uma rede, pois só tem 1 neurônio. Podemos adicionar mais camadas, mais neurônios, mais funções de ativação. No momento que o sistema se torna mais complexo, aí sim estamos falando de uma ANN! E o funcionamento de uma ANN é exatamente o mesmo: defina o número de epochs e sua arquitetura, e então fique repetindo o ciclo de feedfoward-backpropagation. <br>\n",
    "\n",
    "Você pode entender arquiteturas de redes neurais quase como blocos de lego: cada neurônio diferente (ou seja, com estrutura e funções de ativação diferentes) realizará uma função diferente na rede. Elas são tão diversas quanto isso:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/neural_net_zoo.png\" align=\"left\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nossa primeira rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-09682f942011>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Vamos importar o tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# E checar qual versão estamos utilizando.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Escolhemos a 2.0 pois sua sintaxe e funcionamento sem session.run() e\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Vamos importar o tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# E checar qual versão estamos utilizando.\n",
    "# Escolhemos a 2.0 pois sua sintaxe e funcionamento sem session.run() e \n",
    "# construção com base em Keras o torna muito mais intuitivo\n",
    "print(\"Using TensorFlow version\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.2197 - accuracy: 0.9354\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 58us/sample - loss: 0.0962 - accuracy: 0.9711\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0679 - accuracy: 0.9790\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0535 - accuracy: 0.9830\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.0429 - accuracy: 0.9862\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0583 - accuracy: 0.9823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0583485455690301, 0.9823]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O Tensorflow 2.0 já vem com alguns datasets famosos dentro de si.\n",
    "# Vamos importar o MNIST\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# O MNIST do TF 2.0 já vem quebrado em treino e test\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalizando os valores de cada pixel\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Aqui vamos montar nossa rede. Primeiramente, instanciamos um modelo\n",
    "# sequencial. Dentro do modelo, nós podemos colocar as camadas que quisermos\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)), # camada para transformar a imagem 28x28 em um vetor unidimensional\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu), # 512 neuronios fully connected com ativação por ReLU\n",
    "  tf.keras.layers.Dropout(0.2), # Dropout de 20% da última camada\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax) # 10 neurônios softmax, para classificação. Um para cada categoria\n",
    "])\n",
    "\n",
    "# Depois de construida a arquitetura,\n",
    "# precisamos definir otimizador e loss function\n",
    "model.compile(optimizer='adam', # Estamos utilizando o ADAM\n",
    "              loss='sparse_categorical_crossentropy', # nossa loss function é entropia cross-categórica\n",
    "              metrics=['accuracy']) # nossa métrica base será acc. Podemos utilizar mais de uma métrica\n",
    "\n",
    "# Agora que tudo está instanciado, podemos realizar o fit do modelo.\n",
    "# No caso, vamos utilizar 5 epochs.\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
